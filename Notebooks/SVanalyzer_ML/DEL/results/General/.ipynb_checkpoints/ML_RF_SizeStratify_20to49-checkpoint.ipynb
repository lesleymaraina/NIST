{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Machine Learning Training Data : size stratification\n",
    "* Data originally pooled from all technologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lmc2/anaconda/envs/NIHFAES/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/lmc2/anaconda/envs/NIHFAES/lib/python3.5/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import graphviz\n",
    "import io\n",
    "from fancyimpute import KNN\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from scipy.stats import ks_2samp\n",
    "from scipy import stats\n",
    "from matplotlib import pyplot\n",
    "from sklearn import preprocessing\n",
    "from scipy.linalg import svd\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "import plotly.plotly as py\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import f1_score, precision_score\n",
    "from sklearn import preprocessing\n",
    "from ggplot import *\n",
    "from bokeh.charts import TimeSeries\n",
    "from bokeh.models import HoverTool\n",
    "from bokeh.plotting import show\n",
    "from bokeh.charts import Scatter, Histogram, output_file, show\n",
    "from bokeh.plotting import figure, show, output_file, ColumnDataSource\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.charts import Bar, output_file, show\n",
    "import bokeh.palettes as palettes\n",
    "from bokeh.models import HoverTool, BoxSelectTool, Legend\n",
    "from sklearn import (manifold, datasets, decomposition, ensemble,\n",
    "                     discriminant_analysis, random_projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Training Data\n",
    "df_20to49 = pd.read_csv('/Volumes/lesleydata/RF_model_trainingData_test/size_data/train/df_20to49_train_.csv')\n",
    "df_50to99 = pd.read_csv('/Volumes/lesleydata/RF_model_trainingData_test/size_data/train/df_50to99_train_.csv')\n",
    "df_100to299 = pd.read_csv('/Volumes/lesleydata/RF_model_trainingData_test/size_data/train/df_100to299_train_.csv')\n",
    "df_300to399 = pd.read_csv('/Volumes/lesleydata/RF_model_trainingData_test/size_data/train/df_300to399_train_.csv')\n",
    "df_400to499 = pd.read_csv('/Volumes/lesleydata/RF_model_trainingData_test/size_data/train/df_400to499_train_.csv')\n",
    "df_500to999 = pd.read_csv('/Volumes/lesleydata/RF_model_trainingData_test/size_data/train/df_500to999_train_.csv')\n",
    "df_1000to5999 = pd.read_csv('/Volumes/lesleydata/RF_model_trainingData_test/size_data/train/df_1000to5999_train_.csv')\n",
    "df_6000 = pd.read_csv('/Volumes/lesleydata/RF_model_trainingData_test/size_data/train/df_6000_train_.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy Dataframes for later parsing tasks\n",
    "df_20to49_ = pd.read_csv('/Volumes/lesleydata/RF_model_trainingData_test/size_data/train/df_20to49_train.csv')\n",
    "df_50to99_ = pd.read_csv('/Volumes/lesleydata/RF_model_trainingData_test/size_data/train/df_50to99_train.csv')\n",
    "df_100to299_ = pd.read_csv('/Volumes/lesleydata/RF_model_trainingData_test/size_data/train/df_100to299_train.csv')\n",
    "df_300to399_ = pd.read_csv('/Volumes/lesleydata/RF_model_trainingData_test/size_data/train/df_300to399_train.csv')\n",
    "df_400to499_ = pd.read_csv('/Volumes/lesleydata/RF_model_trainingData_test/size_data/train/df_400to499_train.csv')\n",
    "df_500to999_ = pd.read_csv('/Volumes/lesleydata/RF_model_trainingData_test/size_data/train/df_500to999_train.csv')\n",
    "df_1000to5999_ = pd.read_csv('/Volumes/lesleydata/RF_model_trainingData_test/size_data/train/df_1000to5999_train.csv')\n",
    "df_6000_ = pd.read_csv('/Volumes/lesleydata/RF_model_trainingData_test/size_data/train/df_6000_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_20to49['GTcons'].replace('Homozygous_Reference', 0, inplace=True)\n",
    "df_20to49['GTcons'].replace('Heterozygous_Variant', 1, inplace=True)\n",
    "df_20to49['GTcons'].replace('Homozygous_Variant', 2, inplace=True)\n",
    "\n",
    "df_50to99['GTcons'].replace('Homozygous_Reference', 0, inplace=True)\n",
    "df_50to99['GTcons'].replace('Heterozygous_Variant', 1, inplace=True)\n",
    "df_50to99['GTcons'].replace('Homozygous_Variant', 2, inplace=True)\n",
    "\n",
    "df_100to299['GTcons'].replace('Homozygous_Reference', 0, inplace=True)\n",
    "df_100to299['GTcons'].replace('Heterozygous_Variant', 1, inplace=True)\n",
    "df_100to299['GTcons'].replace('Homozygous_Variant', 2, inplace=True)\n",
    "\n",
    "df_300to399['GTcons'].replace('Homozygous_Reference', 0, inplace=True)\n",
    "df_300to399['GTcons'].replace('Heterozygous_Variant', 1, inplace=True)\n",
    "df_300to399['GTcons'].replace('Homozygous_Variant', 2, inplace=True)\n",
    "\n",
    "df_400to499['GTcons'].replace('Homozygous_Reference', 0, inplace=True)\n",
    "df_400to499['GTcons'].replace('Heterozygous_Variant', 1, inplace=True)\n",
    "df_400to499['GTcons'].replace('Homozygous_Variant', 2, inplace=True)\n",
    "\n",
    "df_500to999['GTcons'].replace('Homozygous_Reference', 0, inplace=True)\n",
    "df_500to999['GTcons'].replace('Heterozygous_Variant', 1, inplace=True)\n",
    "df_500to999['GTcons'].replace('Homozygous_Variant', 2, inplace=True)\n",
    "\n",
    "df_1000to5999['GTcons'].replace('Homozygous_Reference', 0, inplace=True)\n",
    "df_1000to5999['GTcons'].replace('Heterozygous_Variant', 1, inplace=True)\n",
    "df_1000to5999['GTcons'].replace('Homozygous_Variant', 2, inplace=True)\n",
    "\n",
    "df_6000['GTcons'].replace('Homozygous_Reference', 0, inplace=True)\n",
    "df_6000['GTcons'].replace('Heterozygous_Variant', 1, inplace=True)\n",
    "df_6000['GTcons'].replace('Homozygous_Variant', 2, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='imbalance'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='hom_ref'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Test Data\n",
    "df_20to49_test = pd.read_csv('/Volumes/lesleydata/RF_model_trainingData_test/size_data/test/df_20to49_test_.csv')\n",
    "df_50to99_test = pd.read_csv('/Volumes/lesleydata/RF_model_trainingData_test/size_data/test/df_50to99_test_.csv')\n",
    "df_100to299_test = pd.read_csv('/Volumes/lesleydata/RF_model_trainingData_test/size_data/test/df_100to299_test_.csv')\n",
    "df_300to399_test = pd.read_csv('/Volumes/lesleydata/RF_model_trainingData_test/size_data/test/df_300to399_test_.csv')\n",
    "df_400to499_test = pd.read_csv('/Volumes/lesleydata/RF_model_trainingData_test/size_data/test/df_400to499_test_.csv')\n",
    "df_500to999_test = pd.read_csv('/Volumes/lesleydata/RF_model_trainingData_test/size_data/test/df_500to999_test_.csv')\n",
    "df_1000to5999_test = pd.read_csv('/Volumes/lesleydata/RF_model_trainingData_test/size_data/test/df_1000to5999_test_.csv')\n",
    "df_6000_test = pd.read_csv('/Volumes/lesleydata/RF_model_trainingData_test/size_data/test/df_6000_test_.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Test Data\n",
    "df_20to49_test_ = pd.read_csv('/Volumes/lesleydata/RF_model_trainingData_test/size_data/test/df_20to49_test.csv')\n",
    "df_50to99_test_ = pd.read_csv('/Volumes/lesleydata/RF_model_trainingData_test/size_data/test/df_50to99_test.csv')\n",
    "df_100to299_test_ = pd.read_csv('/Volumes/lesleydata/RF_model_trainingData_test/size_data/test/df_100to299_test.csv')\n",
    "df_300to399_test_ = pd.read_csv('/Volumes/lesleydata/RF_model_trainingData_test/size_data/test/df_300to399_test.csv')\n",
    "df_400to499_test_ = pd.read_csv('/Volumes/lesleydata/RF_model_trainingData_test/size_data/test/df_400to499_test.csv')\n",
    "df_500to999_test_ = pd.read_csv('/Volumes/lesleydata/RF_model_trainingData_test/size_data/test/df_500to999_test.csv')\n",
    "df_1000to5999_test_ = pd.read_csv('/Volumes/lesleydata/RF_model_trainingData_test/size_data/test/df_1000to5999_test.csv')\n",
    "df_6000_test_ = pd.read_csv('/Volumes/lesleydata/RF_model_trainingData_test/size_data/test/df_6000_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_20to49_test['GTcons'].replace('Homozygous_Reference', 0, inplace=True)\n",
    "df_20to49_test['GTcons'].replace('Heterozygous_Variant', 1, inplace=True)\n",
    "df_20to49_test['GTcons'].replace('Homozygous_Variant', 2, inplace=True)\n",
    "\n",
    "df_50to99_test['GTcons'].replace('Homozygous_Reference', 0, inplace=True)\n",
    "df_50to99_test['GTcons'].replace('Heterozygous_Variant', 1, inplace=True)\n",
    "df_50to99_test['GTcons'].replace('Homozygous_Variant', 2, inplace=True)\n",
    "\n",
    "df_100to299_test['GTcons'].replace('Homozygous_Reference', 0, inplace=True)\n",
    "df_100to299_test['GTcons'].replace('Heterozygous_Variant', 1, inplace=True)\n",
    "df_100to299_test['GTcons'].replace('Homozygous_Variant', 2, inplace=True)\n",
    "\n",
    "df_300to399_test['GTcons'].replace('Homozygous_Reference', 0, inplace=True)\n",
    "df_300to399_test['GTcons'].replace('Heterozygous_Variant', 1, inplace=True)\n",
    "df_300to399_test['GTcons'].replace('Homozygous_Variant', 2, inplace=True)\n",
    "\n",
    "df_400to499_test['GTcons'].replace('Homozygous_Reference', 0, inplace=True)\n",
    "df_400to499_test['GTcons'].replace('Heterozygous_Variant', 1, inplace=True)\n",
    "df_400to499_test['GTcons'].replace('Homozygous_Variant', 2, inplace=True)\n",
    "\n",
    "df_500to999_test['GTcons'].replace('Homozygous_Reference', 0, inplace=True)\n",
    "df_500to999_test['GTcons'].replace('Heterozygous_Variant', 1, inplace=True)\n",
    "df_500to999_test['GTcons'].replace('Homozygous_Variant', 2, inplace=True)\n",
    "\n",
    "df_1000to5999_test['GTcons'].replace('Homozygous_Reference', 0, inplace=True)\n",
    "df_1000to5999_test['GTcons'].replace('Heterozygous_Variant', 1, inplace=True)\n",
    "df_1000to5999_test['GTcons'].replace('Homozygous_Variant', 2, inplace=True)\n",
    "\n",
    "df_6000_test['GTcons'].replace('Homozygous_Reference', 0, inplace=True)\n",
    "df_6000_test['GTcons'].replace('Heterozygous_Variant', 1, inplace=True)\n",
    "df_6000_test['GTcons'].replace('Homozygous_Variant', 2, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Store header names in lists and find names that are NOT contained in BOTH lists\n",
    "# c = list(df_train.columns.values)\n",
    "# d = list(df_test.columns.values)\n",
    "# set(d) - set(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**20-49bp**\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Impute missing values using KNN\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GTcons</th>\n",
       "      <th>Ill250.alt_alnScore_mean</th>\n",
       "      <th>Ill250.alt_alnScore_std</th>\n",
       "      <th>Ill250.alt_count</th>\n",
       "      <th>Ill250.alt_insertSize_mean</th>\n",
       "      <th>Ill250.alt_insertSize_std</th>\n",
       "      <th>Ill250.alt_reason_alignmentScore</th>\n",
       "      <th>Ill250.alt_reason_insertSizeScore</th>\n",
       "      <th>Ill250.alt_reason_orientation</th>\n",
       "      <th>Ill250.amb_alnScore_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>pacbio.ref_insertSize_mean</th>\n",
       "      <th>pacbio.ref_insertSize_std</th>\n",
       "      <th>pacbio.ref_reason_alignmentScore</th>\n",
       "      <th>refN_cnt</th>\n",
       "      <th>refN_pct</th>\n",
       "      <th>segdup_cnt</th>\n",
       "      <th>segdup_pct</th>\n",
       "      <th>start</th>\n",
       "      <th>tandemrep_cnt</th>\n",
       "      <th>tandemrep_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>976.871795</td>\n",
       "      <td>12.113098</td>\n",
       "      <td>39.0</td>\n",
       "      <td>431.384615</td>\n",
       "      <td>91.092406</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>890.273292</td>\n",
       "      <td>...</td>\n",
       "      <td>8754.50</td>\n",
       "      <td>3356.194618</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65359519</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>907.097143</td>\n",
       "      <td>...</td>\n",
       "      <td>10767.32</td>\n",
       "      <td>3791.304854</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44564287</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>980.865672</td>\n",
       "      <td>12.845692</td>\n",
       "      <td>67.0</td>\n",
       "      <td>430.701492</td>\n",
       "      <td>88.234426</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>883.863905</td>\n",
       "      <td>...</td>\n",
       "      <td>18545.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>96806440</td>\n",
       "      <td>1</td>\n",
       "      <td>0.586207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 177 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   GTcons  Ill250.alt_alnScore_mean  Ill250.alt_alnScore_std  \\\n",
       "0       1                976.871795                12.113098   \n",
       "1       0                  0.000000                 0.000000   \n",
       "2       1                980.865672                12.845692   \n",
       "\n",
       "   Ill250.alt_count  Ill250.alt_insertSize_mean  Ill250.alt_insertSize_std  \\\n",
       "0              39.0                  431.384615                  91.092406   \n",
       "1               0.0                    0.000000                   0.000000   \n",
       "2              67.0                  430.701492                  88.234426   \n",
       "\n",
       "   Ill250.alt_reason_alignmentScore  Ill250.alt_reason_insertSizeScore  \\\n",
       "0                              39.0                                0.0   \n",
       "1                               0.0                                0.0   \n",
       "2                              67.0                                0.0   \n",
       "\n",
       "   Ill250.alt_reason_orientation  Ill250.amb_alnScore_mean      ...        \\\n",
       "0                            0.0                890.273292      ...         \n",
       "1                            0.0                907.097143      ...         \n",
       "2                            0.0                883.863905      ...         \n",
       "\n",
       "   pacbio.ref_insertSize_mean  pacbio.ref_insertSize_std  \\\n",
       "0                     8754.50                3356.194618   \n",
       "1                    10767.32                3791.304854   \n",
       "2                    18545.00                   0.000000   \n",
       "\n",
       "   pacbio.ref_reason_alignmentScore  refN_cnt  refN_pct  segdup_cnt  \\\n",
       "0                              30.0         0         0           0   \n",
       "1                              50.0         0         0           0   \n",
       "2                               1.0         0         0           1   \n",
       "\n",
       "   segdup_pct     start  tandemrep_cnt  tandemrep_pct  \n",
       "0         0.0  65359519              0       0.000000  \n",
       "1         0.0  44564287              1       1.000000  \n",
       "2         1.0  96806440              1       0.586207  \n",
       "\n",
       "[3 rows x 177 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store training data in a new variable which will be converted to a matrix\n",
    "X = df_20to49\n",
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing row 1/1581 with 1 missing, elapsed time: 2.152\n",
      "Imputing row 101/1581 with 1 missing, elapsed time: 2.232\n",
      "Imputing row 201/1581 with 2 missing, elapsed time: 2.244\n",
      "Imputing row 301/1581 with 2 missing, elapsed time: 2.256\n",
      "Imputing row 401/1581 with 2 missing, elapsed time: 2.269\n",
      "Imputing row 501/1581 with 2 missing, elapsed time: 2.283\n",
      "Imputing row 601/1581 with 1 missing, elapsed time: 2.294\n",
      "Imputing row 701/1581 with 2 missing, elapsed time: 2.306\n",
      "Imputing row 801/1581 with 1 missing, elapsed time: 2.318\n",
      "Imputing row 901/1581 with 58 missing, elapsed time: 2.329\n",
      "Imputing row 1001/1581 with 2 missing, elapsed time: 2.338\n",
      "Imputing row 1101/1581 with 1 missing, elapsed time: 2.352\n",
      "Imputing row 1201/1581 with 2 missing, elapsed time: 2.365\n",
      "Imputing row 1301/1581 with 2 missing, elapsed time: 2.381\n",
      "Imputing row 1401/1581 with 1 missing, elapsed time: 2.397\n",
      "Imputing row 1501/1581 with 2 missing, elapsed time: 2.417\n"
     ]
    }
   ],
   "source": [
    "# Convert dataframe to matrix\n",
    "X=X.as_matrix()\n",
    "\n",
    "#Imput missing values from three closest observations\n",
    "X_imputed=KNN(k=3).complete(X)\n",
    "X=pd.DataFrame(X_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GTcons</th>\n",
       "      <th>Ill250.alt_alnScore_mean</th>\n",
       "      <th>Ill250.alt_alnScore_std</th>\n",
       "      <th>Ill250.alt_count</th>\n",
       "      <th>Ill250.alt_insertSize_mean</th>\n",
       "      <th>Ill250.alt_insertSize_std</th>\n",
       "      <th>Ill250.alt_reason_alignmentScore</th>\n",
       "      <th>Ill250.alt_reason_insertSizeScore</th>\n",
       "      <th>Ill250.alt_reason_orientation</th>\n",
       "      <th>Ill250.amb_alnScore_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>pacbio.ref_insertSize_mean</th>\n",
       "      <th>pacbio.ref_insertSize_std</th>\n",
       "      <th>pacbio.ref_reason_alignmentScore</th>\n",
       "      <th>refN_cnt</th>\n",
       "      <th>refN_pct</th>\n",
       "      <th>segdup_cnt</th>\n",
       "      <th>segdup_pct</th>\n",
       "      <th>start</th>\n",
       "      <th>tandemrep_cnt</th>\n",
       "      <th>tandemrep_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>976.871795</td>\n",
       "      <td>12.113098</td>\n",
       "      <td>39.0</td>\n",
       "      <td>431.384615</td>\n",
       "      <td>91.092406</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>890.273292</td>\n",
       "      <td>...</td>\n",
       "      <td>8754.50</td>\n",
       "      <td>3356.194618</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65359519.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>907.097143</td>\n",
       "      <td>...</td>\n",
       "      <td>10767.32</td>\n",
       "      <td>3791.304854</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44564287.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>980.865672</td>\n",
       "      <td>12.845692</td>\n",
       "      <td>67.0</td>\n",
       "      <td>430.701492</td>\n",
       "      <td>88.234426</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>883.863905</td>\n",
       "      <td>...</td>\n",
       "      <td>18545.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>96806440.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.586207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 177 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   GTcons  Ill250.alt_alnScore_mean  Ill250.alt_alnScore_std  \\\n",
       "0     1.0                976.871795                12.113098   \n",
       "1     0.0                  0.000000                 0.000000   \n",
       "2     1.0                980.865672                12.845692   \n",
       "\n",
       "   Ill250.alt_count  Ill250.alt_insertSize_mean  Ill250.alt_insertSize_std  \\\n",
       "0              39.0                  431.384615                  91.092406   \n",
       "1               0.0                    0.000000                   0.000000   \n",
       "2              67.0                  430.701492                  88.234426   \n",
       "\n",
       "   Ill250.alt_reason_alignmentScore  Ill250.alt_reason_insertSizeScore  \\\n",
       "0                              39.0                                0.0   \n",
       "1                               0.0                                0.0   \n",
       "2                              67.0                                0.0   \n",
       "\n",
       "   Ill250.alt_reason_orientation  Ill250.amb_alnScore_mean      ...        \\\n",
       "0                            0.0                890.273292      ...         \n",
       "1                            0.0                907.097143      ...         \n",
       "2                            0.0                883.863905      ...         \n",
       "\n",
       "   pacbio.ref_insertSize_mean  pacbio.ref_insertSize_std  \\\n",
       "0                     8754.50                3356.194618   \n",
       "1                    10767.32                3791.304854   \n",
       "2                    18545.00                   0.000000   \n",
       "\n",
       "   pacbio.ref_reason_alignmentScore  refN_cnt  refN_pct  segdup_cnt  \\\n",
       "0                              30.0       0.0       0.0         0.0   \n",
       "1                              50.0       0.0       0.0         0.0   \n",
       "2                               1.0       0.0       0.0         1.0   \n",
       "\n",
       "   segdup_pct       start  tandemrep_cnt  tandemrep_pct  \n",
       "0         0.0  65359519.0            0.0       0.000000  \n",
       "1         0.0  44564287.0            1.0       1.000000  \n",
       "2         1.0  96806440.0            1.0       0.586207  \n",
       "\n",
       "[3 rows x 177 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store header values in a list, will be used later to re-label the matrix post KNN imputation\n",
    "df_20to49_header = list(df_20to49.columns.values)\n",
    "X.columns = df_20to49_header\n",
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store Labels in a new 'Y' DataFrame\n",
    "Y = pd.DataFrame()\n",
    "Y = X['GTcons']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    855\n",
       "0.0    453\n",
       "2.0    273\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Count the number of labels\n",
    "pd.value_counts(Y.values, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove labels from feature set\n",
    "X.drop(['GTcons'],axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Order features\n",
    "X4 = X.reindex_axis(sorted(X.columns), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Machine Learning\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='machine_learning'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Train Random Forest Classifier **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='multi_run'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Determine Number of trees: Out of Bag Error **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "# Train on 70% of the data and test on 30%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X4, Y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lmc2/anaconda/envs/NIHFAES/lib/python3.5/site-packages/sklearn/ensemble/forest.py:439: UserWarning:\n",
      "\n",
      "Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=True, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForestClassifier(oob_score=True) \n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE :** \n",
    "\n",
    "Determined OOB score based on 70% of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lmc2/anaconda/envs/NIHFAES/lib/python3.5/site-packages/sklearn/ensemble/forest.py:439: UserWarning:\n",
      "\n",
      "Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "\n",
      "/Users/lmc2/anaconda/envs/NIHFAES/lib/python3.5/site-packages/sklearn/ensemble/forest.py:439: UserWarning:\n",
      "\n",
      "Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "\n",
      "/Users/lmc2/anaconda/envs/NIHFAES/lib/python3.5/site-packages/sklearn/ensemble/forest.py:439: UserWarning:\n",
      "\n",
      "Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "\n",
      "/Users/lmc2/anaconda/envs/NIHFAES/lib/python3.5/site-packages/sklearn/ensemble/forest.py:439: UserWarning:\n",
      "\n",
      "Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "\n",
      "/Users/lmc2/anaconda/envs/NIHFAES/lib/python3.5/site-packages/sklearn/ensemble/forest.py:439: UserWarning:\n",
      "\n",
      "Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "\n",
      "/Users/lmc2/anaconda/envs/NIHFAES/lib/python3.5/site-packages/sklearn/ensemble/forest.py:439: UserWarning:\n",
      "\n",
      "Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "\n",
      "/Users/lmc2/anaconda/envs/NIHFAES/lib/python3.5/site-packages/sklearn/ensemble/forest.py:439: UserWarning:\n",
      "\n",
      "Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "\n",
      "/Users/lmc2/anaconda/envs/NIHFAES/lib/python3.5/site-packages/sklearn/ensemble/forest.py:439: UserWarning:\n",
      "\n",
      "Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "\n",
      "/Users/lmc2/anaconda/envs/NIHFAES/lib/python3.5/site-packages/sklearn/ensemble/forest.py:439: UserWarning:\n",
      "\n",
      "Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "\n",
      "/Users/lmc2/anaconda/envs/NIHFAES/lib/python3.5/site-packages/sklearn/ensemble/forest.py:439: UserWarning:\n",
      "\n",
      "Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "\n",
      "/Users/lmc2/anaconda/envs/NIHFAES/lib/python3.5/site-packages/sklearn/ensemble/forest.py:439: UserWarning:\n",
      "\n",
      "Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "\n",
      "/Users/lmc2/anaconda/envs/NIHFAES/lib/python3.5/site-packages/sklearn/ensemble/forest.py:439: UserWarning:\n",
      "\n",
      "Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "\n",
      "/Users/lmc2/anaconda/envs/NIHFAES/lib/python3.5/site-packages/sklearn/ensemble/forest.py:439: UserWarning:\n",
      "\n",
      "Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "\n",
      "/Users/lmc2/anaconda/envs/NIHFAES/lib/python3.5/site-packages/sklearn/ensemble/forest.py:439: UserWarning:\n",
      "\n",
      "Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "\n",
      "/Users/lmc2/anaconda/envs/NIHFAES/lib/python3.5/site-packages/sklearn/ensemble/forest.py:439: UserWarning:\n",
      "\n",
      "Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "\n",
      "/Users/lmc2/anaconda/envs/NIHFAES/lib/python3.5/site-packages/sklearn/ensemble/forest.py:439: UserWarning:\n",
      "\n",
      "Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "\n",
      "/Users/lmc2/anaconda/envs/NIHFAES/lib/python3.5/site-packages/sklearn/ensemble/forest.py:439: UserWarning:\n",
      "\n",
      "Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "\n",
      "/Users/lmc2/anaconda/envs/NIHFAES/lib/python3.5/site-packages/sklearn/ensemble/forest.py:439: UserWarning:\n",
      "\n",
      "Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "\n",
      "/Users/lmc2/anaconda/envs/NIHFAES/lib/python3.5/site-packages/sklearn/ensemble/forest.py:439: UserWarning:\n",
      "\n",
      "Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "\n",
      "/Users/lmc2/anaconda/envs/NIHFAES/lib/python3.5/site-packages/sklearn/ensemble/forest.py:439: UserWarning:\n",
      "\n",
      "Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "\n",
      "/Users/lmc2/anaconda/envs/NIHFAES/lib/python3.5/site-packages/sklearn/ensemble/forest.py:439: UserWarning:\n",
      "\n",
      "Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "\n",
      "/Users/lmc2/anaconda/envs/NIHFAES/lib/python3.5/site-packages/sklearn/ensemble/forest.py:439: UserWarning:\n",
      "\n",
      "Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "\n",
      "/Users/lmc2/anaconda/envs/NIHFAES/lib/python3.5/site-packages/sklearn/ensemble/forest.py:439: UserWarning:\n",
      "\n",
      "Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "\n",
      "/Users/lmc2/anaconda/envs/NIHFAES/lib/python3.5/site-packages/sklearn/ensemble/forest.py:439: UserWarning:\n",
      "\n",
      "Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "\n",
      "/Users/lmc2/anaconda/envs/NIHFAES/lib/python3.5/site-packages/sklearn/ensemble/forest.py:439: UserWarning:\n",
      "\n",
      "Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "\n",
      "/Users/lmc2/anaconda/envs/NIHFAES/lib/python3.5/site-packages/sklearn/ensemble/forest.py:439: UserWarning:\n",
      "\n",
      "Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "\n",
      "/Users/lmc2/anaconda/envs/NIHFAES/lib/python3.5/site-packages/sklearn/ensemble/forest.py:439: UserWarning:\n",
      "\n",
      "Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "\n",
      "/Users/lmc2/anaconda/envs/NIHFAES/lib/python3.5/site-packages/sklearn/ensemble/forest.py:439: UserWarning:\n",
      "\n",
      "Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "\n",
      "/Users/lmc2/anaconda/envs/NIHFAES/lib/python3.5/site-packages/sklearn/ensemble/forest.py:439: UserWarning:\n",
      "\n",
      "Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "\n",
      "/Users/lmc2/anaconda/envs/NIHFAES/lib/python3.5/site-packages/sklearn/ensemble/forest.py:439: UserWarning:\n",
      "\n",
      "Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "\n",
      "/Users/lmc2/anaconda/envs/NIHFAES/lib/python3.5/site-packages/sklearn/ensemble/forest.py:439: UserWarning:\n",
      "\n",
      "Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "n_estimators = 500\n",
    "trees = []\n",
    "oob = []\n",
    "for i in range(1, n_estimators):\n",
    "    model.set_params(n_estimators=i)\n",
    "    model.fit(X_train, y_train)\n",
    "    trees += [i]\n",
    "    oob += [model.oob_score_] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_oob = pd.DataFrame()\n",
    "df_oob['trees'] = trees\n",
    "df_oob['oob'] = oob\n",
    "\n",
    "\n",
    "plt.plot(trees, oob)\n",
    "\n",
    "plt.xlabel(\"Number of Trees [n_estimators]\")\n",
    "plt.ylabel(\"OOB error rate\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.savefig('/Volumes/lesleydata/RF_model_trainingData_test/size_data/results/figures/20-45bp_OOB.png', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Train Model Using Optimal Tuning Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=500, random_state=4) \n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Another Resource](https://stackoverflow.com/questions/37877542/how-to-label-the-feature-importance-with-forests-of-trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='100_trees'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "feature_importances_2 = pd.Series(model.feature_importances_, index=X_train.columns)\n",
    "feature_importances_2.sort()\n",
    "plt.figure(figsize=(37, 37))\n",
    "plt.savefig('/Volumes/lesleydata/RF_model_trainingData_test/size_data/results/figuresalldata_features_100trees_.png', bbox_inches='tight')\n",
    "feature_importances_2.plot(kind=\"barh\", figsize=(8,30))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: Training Set - Show number of Hom Ref, Hom Var, Het Var datapoints the model was trained on\n",
    "ytrain = pd.DataFrame()\n",
    "ytrain['ytrain'] = y_train\n",
    "pd.value_counts(ytrain['ytrain'].values, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='prediction_step'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='traintest_precision'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Precision score of the training subset: {:.3f}'.format(precision_score(y_test, pred, average='micro'))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy score of the training subset: {:.3f}'.format(accuracy_score(y_test, pred))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add original labels and predicted labels back to the original dataframe\n",
    "df_Xtest = pd.DataFrame(X_test)\n",
    "df_Xtest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = pd.DataFrame(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_Xtest['predicted_label'] = pred\n",
    "df_Xtest['GTcons'] = df_20to49['GTcons']\n",
    "df_Xtest['chrom'] = df_20to49['chrom']\n",
    "df_Xtest['start'] = df_20to49['start']\n",
    "df_Xtest['end'] = df_20to49['end']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_Xtest['GTcons'].replace(0.0, 'Homozygous_Reference', inplace=True)\n",
    "df_Xtest['GTcons'].replace(1.0, 'Heterozygous_Variant', inplace=True)\n",
    "df_Xtest['GTcons'].replace(2.0, 'Homozygous_Variant', inplace=True)\n",
    "df_Xtest['predicted_label'].replace(0.0, 'Homozygous_Reference', inplace=True)\n",
    "df_Xtest['predicted_label'].replace(1.0, 'Heterozygous_Variant', inplace=True)\n",
    "df_Xtest['predicted_label'].replace(2.0, 'Homozygous_Variant', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(df_Xtest['GTcons'].values, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(df_Xtest['predicted_label'].values, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "ytest = df_Xtest['GTcons']\n",
    "predict = df_Xtest['predicted_label']\n",
    "print(confusion_matrix(ytest, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='traintest_confusion_matrix'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(ytest, predict, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(ytest, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Predict\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='predict'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description:\n",
    "\n",
    "   * In the [previous section](#machine_learning), a RF model was trained on svanalyzer data.\n",
    "\n",
    "       * The model was trained using [train/test split](#train_test) where 70% of the data was used to train the model and the model performance was determined by predicting labels for the remaining 30% of the data\n",
    " * Reminder: The labels for this training set and the following [prediction step](#prediction_step) are the consensus genotype (GTcons) labels generated from a preliminary R analysis based on reference and alternate read count:\n",
    "           * Homozygous Reference (0)\n",
    "           * Heterozygous Variant (1)\n",
    "           * Homozygous Variant (2)\n",
    "           \n",
    "   * The trained model is used in the following section to predict labels for 5000 randomly selected Deletions [these datapoints were randomly selected from [union_170509_refalt.sort.vcf](ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/analysis/NIST_UnionSVs_05092017/)]\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Load Data **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X2 = df_20to49_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Impute missing values using KNN **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert dataframe to matrix\n",
    "X2=X2.as_matrix()\n",
    "X2=pd.DataFrame(X2)\n",
    "\n",
    "# Imput missing values from three closest observations\n",
    "X2_imputed=KNN(k=3).complete(X2)\n",
    "X2=pd.DataFrame(X2_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: Error above notes that there are no missing values **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_20to49_test_header = list(df_20to49_test.columns.values)\n",
    "X2.columns = df_20to49_test_header\n",
    "X2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3 = pd.DataFrame()\n",
    "X3 = X2\n",
    "X3.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Order features\n",
    "X5 = X2.reindex_axis(sorted(X2.columns), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# X5 = df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='prediction_step'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = model.predict(X5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_prob = model.predict_proba(X5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_prob_log = model.predict_log_proba(X5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X5['predicted_label'] = pred\n",
    "X5['chrom'] = df_20to49_test_['chrom']\n",
    "X5['GTcons'] = df_20to49_test_['GTcons']\n",
    "X5['start'] = df_20to49_test_['start']\n",
    "X5['end'] = df_20to49_test_['end']\n",
    "X5['Size'] = df_20to49_test_['Size']\n",
    "X5['GTsupp'] = df_20to49_test_['GTsupp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X6 = pd.concat([X5, pd.DataFrame(pred_prob, columns=['1','2','3'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X7 = pd.concat([X5, pd.DataFrame(pred_prob_log, columns=['1','2','3'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X6.to_csv('/Volumes/lesleydata/RF_model_trainingData_test/size_data/results/df_20to49.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X7.to_csv('/Volumes/lesleydata/RF_model_trainingData_test/size_data/results/df_20to49_log.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X6 = pd.read_csv('/Volumes/lesleydata/RF_model_trainingData_test/size_data/results/df_20to49.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X6.rename(columns={'1': 'Homozygous_Reference_GTcons'}, inplace=True)\n",
    "X6.rename(columns={'2': 'Heterozygous_Variant_GTcons'}, inplace=True)\n",
    "X6.rename(columns={'3': 'Homozygous_Variant_GTcons'}, inplace=True)\n",
    "X6.rename(columns={'predicted_label': 'predicted_GTcons_label'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X6.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X6.to_csv('/Volumes/lesleydata/RF_model_trainingData_test/size_data/results/df_20to49_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Label Analysis\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description:\n",
    "  * The [random forest(RF) model](#train_test) was trained on svanalyzer data. The trained model was used to predict consensus GT labels for the 5000 deletions that were randomly selected from the union_refalt vcf. The following is a comparison of model predicted labels [Conesus GT] to consensus genotype generated by the R script for the 5000 randomly selected datapoints from union_refalt.vcf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "consensus_GT = X6['GTcons']\n",
    "predict = X6['predicted_GTcons_label']\n",
    "print(confusion_matrix(consensus_GT, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X6['GTcons'].replace(0, 'Homozygous_Reference', inplace=True)\n",
    "X6['GTcons'].replace(1, 'Heterozygous_Variant', inplace=True)\n",
    "X6['GTcons'].replace(2, 'Homozygous_Variant', inplace=True)\n",
    "X6['predicted_GTcons_label'].replace(0.0, 'Homozygous_Reference', inplace=True)\n",
    "X6['predicted_GTcons_label'].replace(1.0, 'Heterozygous_Variant', inplace=True)\n",
    "X6['predicted_GTcons_label'].replace(2.0, 'Homozygous_Variant', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='predict_precision_score_matrix'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Precision score of the prediction subset: {:.3f}'.format(precision_score(consensus_GT, predict, average='micro'))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(consensus_GT, predict, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** High Confidence Label Analysis**\n",
    "* **Reminder:** The labels predicted by the model are the following consensus genotype:\n",
    "    * Homozygous Reference: 0 \n",
    "    * Heterozygous Variant: 1 \n",
    "    * Homozygous Variant: 2 \n",
    "* Here **high confidence labels** are the GTcons labels predicted by the model that were also assigned a predict probability of either 0.9 or 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "high_conf_labels = X6[(X6['Homozygous_Reference_GTcons'] == 1) | (X6['Homozygous_Reference_GTcons'] >= 0.9) | (X6['Heterozygous_Variant_GTcons'] == 1) | (X6['Heterozygous_Variant_GTcons'] >= 0.9) | (X6['Homozygous_Variant_GTcons'] == 1) | (X6['Homozygous_Variant_GTcons'] >= 0.9)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='hiconf_precision_score'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consensus_GT = high_conf_labels['GTcons']\n",
    "predict = high_conf_labels['predicted_GTcons_label']\n",
    "pd.crosstab(consensus_GT, predict, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(consensus_GT, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [NIHFAES]",
   "language": "python",
   "name": "Python [NIHFAES]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
